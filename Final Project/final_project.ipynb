{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T18:23:46.344698Z",
     "start_time": "2025-06-25T18:23:46.167459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pyfeats\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ],
   "id": "7928752fa7a40171",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-24T12:21:13.847825Z",
     "start_time": "2025-06-24T12:21:13.840446Z"
    }
   },
   "source": [
    "def extract_all_pyfeats(img_path):\n",
    "    # Load image as grayscale\n",
    "    f = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if f is None:\n",
    "        raise ValueError(\"Image not found or failed to load\")\n",
    "\n",
    "    # Create binary mask\n",
    "    mask = np.ones_like(f, dtype=bool)\n",
    "\n",
    "    # Extract all supported features\n",
    "    fos_feats, _ = pyfeats.fos(f, mask)\n",
    "    glcm_feats, _, _, _  = pyfeats.glcm_features(f)\n",
    "    glds_feats, _ = pyfeats.glds_features(f, mask)\n",
    "    ngtdm_feats, _ = pyfeats.ngtdm_features(f, mask)\n",
    "    sfm_feats, _ = pyfeats.sfm_features(f, mask)\n",
    "    fdta_feats, _ = pyfeats.fdta(f, mask)\n",
    "    glrlm_feats, _ = pyfeats.glrlm_features(f, mask)\n",
    "    fps_feats, _ = pyfeats.fps(f, mask)\n",
    "    glszm_feats, _ = pyfeats.glszm_features(f, mask)\n",
    "    lbp_feats, _ = pyfeats.lbp_features(f, mask)\n",
    "    amfm_feats, _ = pyfeats.amfm_features(f)\n",
    "   # dwt_feats, _ = pyfeats.dwt_features(f, mask)\n",
    "    swt_feats, _ = pyfeats.swt_features(f, mask)\n",
    "    wp_feats, _ = pyfeats.wp_features(f, mask)\n",
    "    gt_feats, _ = pyfeats.gt_features(f, mask)\n",
    "    hu_feats, _ = pyfeats.hu_moments(f)\n",
    "    tas_feats, _ = pyfeats.tas_features(f)\n",
    "    hog_feats, _ = pyfeats.hog_features(f)\n",
    "    hist_feats, _ = pyfeats.histogram(f, mask)\n",
    "    mrh_feats, _  = pyfeats.multiregion_histogram(f, mask)\n",
    "    lte_feats, _ = pyfeats.lte_measures(f, mask)\n",
    "    zern_feats, _ = pyfeats.zernikes_moments(f, mask)\n",
    "\n",
    "    # Reduce GLCM to summary\n",
    "    glcm_mean = np.mean(glcm_feats)\n",
    "    glcm_range = np.ptp(glcm_feats)\n",
    "\n",
    "    # Combine into one vector\n",
    "    features = np.concatenate([\n",
    "        fos_feats,\n",
    "        [glcm_mean, glcm_range],\n",
    "        glds_feats,\n",
    "        ngtdm_feats,\n",
    "        sfm_feats,\n",
    "        lte_feats,\n",
    "        fdta_feats,\n",
    "        glrlm_feats,\n",
    "        fps_feats,\n",
    "        glszm_feats,\n",
    "        lbp_feats,\n",
    "        hist_feats,\n",
    "        mrh_feats,\n",
    "        amfm_feats,\n",
    "       # dwt_feats,\n",
    "        swt_feats,\n",
    "        wp_feats,\n",
    "        gt_feats,\n",
    "        zern_feats,\n",
    "        hu_feats,\n",
    "        tas_feats,\n",
    "        hog_feats\n",
    "    ])\n",
    "\n",
    "    # Clean NaN or Inf if exist\n",
    "    return np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T12:21:13.863253Z",
     "start_time": "2025-06-24T12:21:13.857931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fake_test = r\"C:\\Users\\Admin\\PycharmProjects\\SchoolStuff\\MachineLerning\\Final_Project\\test\\FAKE\"\n",
    "fake_train = r\"C:\\Users\\Admin\\PycharmProjects\\SchoolStuff\\MachineLerning\\Final_Project\\train\\FAKE\"\n",
    "real_test = r\"C:\\Users\\Admin\\PycharmProjects\\SchoolStuff\\MachineLerning\\Final_Project\\test\\REAL\"\n",
    "real_train = r\"C:\\Users\\Admin\\PycharmProjects\\SchoolStuff\\MachineLerning\\Final_Project\\train\\REAL\""
   ],
   "id": "806bf8f7300e009f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import os\n",
    "#\n",
    "# image_extensions = ('.jpg', '.jpeg', '.png')\n",
    "# x_train = []\n",
    "#\n",
    "# for filename in os.listdir(real_test):\n",
    "#     if filename.lower().endswith(image_extensions):\n",
    "#         image_path = os.path.join(real_test, filename)\n",
    "#         try:\n",
    "#             features = extract_all_pyfeats(image_path)\n",
    "#             x_train.append(features)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {filename}: {e}\")\n",
    "#\n",
    "# # Convert list to numpy array\n",
    "# x_train = np.array(x_train)\n",
    "#\n",
    "# # Save to file\n",
    "# np.save(\"x_real_test.npy\", x_train)\n"
   ],
   "id": "b0875c275ac3997f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Node class",
   "id": "cdb93045ff362d6b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T07:26:45.458870Z",
     "start_time": "2025-06-25T07:26:45.454150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None"
   ],
   "id": "1cae3b992ac532b9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Decision Tree",
   "id": "de0c11ea546cf15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T07:26:45.481863Z",
     "start_time": "2025-06-25T07:26:45.466840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split = 2, max_depth = 20, features = None, moi = \"gini\"):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.features = features\n",
    "        self.root = None\n",
    "        self.MOI = moi.lower() if moi.lower() in (\"gini\", \"entropy\") else \"gini\" # metric of impurity\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # X.shape[1] number of features\n",
    "        self.features = X.shape[1] if self.features is None else min(self.features, X.shape[1])\n",
    "        self.root = self.build_tree(X, y)\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        x_samples, x_features = X.shape\n",
    "        y_labels = len(np.unique(y))\n",
    "\n",
    "        if depth >= self.max_depth or y_labels == 1 or x_samples < self.min_samples_split:\n",
    "            return Node(value=np.bincount(y).argmax())\n",
    "\n",
    "        feature_indexes = np.random.choice(x_features, self.features, replace=False)\n",
    "\n",
    "        best_threshold, best_feature = self.best_split(X, y, feature_indexes)\n",
    "        if best_threshold is None or best_feature is None:\n",
    "            return Node(value=np.bincount(y).argmax())\n",
    "\n",
    "        left_indexes, right_indexes = self.split(X[:, best_feature], best_threshold)\n",
    "\n",
    "        left_subtree = self.build_tree(X[left_indexes], y[left_indexes], depth=depth+1)\n",
    "        right_subtree = self.build_tree(X[right_indexes], y[right_indexes], depth=depth+1)\n",
    "        return Node(best_feature, best_threshold, left_subtree, right_subtree)\n",
    "\n",
    "    def best_split(self, X, y, feature_indexes):\n",
    "        best_gain = -1\n",
    "        best_threshold, best_feature = None, None\n",
    "        gini_parent = self.gini(y)\n",
    "        entropy_parent = self.entropy(y)\n",
    "        n = len(y)\n",
    "\n",
    "        for feature_index in feature_indexes:\n",
    "            X_column = X[:, feature_index]\n",
    "            thresholds = X_column\n",
    "            for threshold in thresholds:\n",
    "                left_indexes, right_indexes = self.split(X_column, threshold)\n",
    "\n",
    "                if len(left_indexes) == 0 or len(right_indexes) == 0:\n",
    "                    gain = 0\n",
    "\n",
    "                elif self.MOI.lower() == \"entropy\":\n",
    "                    entropy_left = self.gini(y[left_indexes])\n",
    "                    entropy_right = self.gini(y[right_indexes])\n",
    "                    weighted_entropy = (len(left_indexes) * entropy_left + len(right_indexes) * entropy_right) / n\n",
    "                    gain = entropy_parent - weighted_entropy\n",
    "\n",
    "                else:\n",
    "                    gini_left = self.gini(y[left_indexes])\n",
    "                    gini_right = self.gini(y[right_indexes])\n",
    "                    weighted_gini = (len(left_indexes) * gini_left + len(right_indexes) * gini_right) / n\n",
    "                    gain = gini_parent - weighted_gini\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_threshold = threshold\n",
    "                    best_feature = feature_index\n",
    "\n",
    "        return best_threshold, best_feature\n",
    "\n",
    "    def split(self, X_column, threshold):\n",
    "        left_indexes = np.where(X_column <= threshold)[0].flatten()\n",
    "        right_indexes = np.where(X_column > threshold)[0].flatten()\n",
    "        return left_indexes, right_indexes\n",
    "\n",
    "    def create_probabilitys(self, y):\n",
    "        return np.unique(y, return_counts=True)[1] / len(y)\n",
    "\n",
    "    def gini(self, y):\n",
    "        return 1 - np.sum(self.create_probabilitys(y) ** 2)\n",
    "\n",
    "    def entropy(self, y):\n",
    "        return -np.sum([p * np.log(p) for p in self.create_probabilitys(y) if p>0])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def traverse_tree(self, X, node):\n",
    "        \"\"\"\n",
    "            if node.is_leaf_node():\n",
    "                return node.value\n",
    "\n",
    "            return self.traverse_tree(X, node.left) if X[node.feature] <= node.threshold else self.traverse_tree(X, node.right)\n",
    "\n",
    "        \"\"\"\n",
    "        return node.value if node.is_leaf_node() else self.traverse_tree(X, node.left) if X[node.feature] <= node.threshold else self.traverse_tree(X, node.right)"
   ],
   "id": "38e8ee6f665dd99e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Random Forest",
   "id": "da8647bdf4b78b46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T07:26:45.497813Z",
     "start_time": "2025-06-25T07:26:45.489867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RandomForest:\n",
    "    def __init__(self,n_trees = 100 ,min_samples_split = 2, max_depth = 20, features = None, moi = \"gini\"):\n",
    "        self.n_trees = n_trees\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.features = features\n",
    "        self.trees = []\n",
    "        self.MOI = moi.lower() if moi.lower() in (\"gini\", \"entropy\") else \"gini\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_trees):\n",
    "            decision_tree = DecisionTree(self.min_samples_split, self.max_depth, self.features, self.MOI)\n",
    "            x_sub, y_sub = self.bootstrap_sample(X, y)\n",
    "            decision_tree.fit(x_sub, y_sub)\n",
    "            self.trees.append(decision_tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            labels = [tree.predict(x.reshape(1, -1))[0] for tree in self.trees]\n",
    "            common_label = np.bincount(labels).argmax()\n",
    "            predictions.append(common_label)\n",
    "        return predictions\n",
    "\n",
    "    def bootstrap_sample(self, X, y):\n",
    "        samples, _ = X.shape\n",
    "        sample_indexes = np.random.choice(samples, samples, replace=True)\n",
    "        return X[sample_indexes], y[sample_indexes]"
   ],
   "id": "d497e56888ee94ba",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## AdaBoost",
   "id": "db19243e157322a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T07:26:45.911698Z",
     "start_time": "2025-06-25T07:26:45.906064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, min_samples_split = 2, estimators = 50, learning_rate = 1, features = None, moi = \"gini\"):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.estimators = estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.features = features\n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "        self.MOI = moi.lower() if moi.lower() in (\"gini\", \"entropy\") else \"gini\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = len(X)\n",
    "        sample_weights = np.full(n_samples, 1 / n_samples)\n",
    "\n",
    "        for _ in range(self.estimators):\n",
    "            sample_indexes = np.random.choice(n_samples, n_samples, replace=True, p=sample_weights)\n",
    "            X_sub = X[sample_indexes]\n",
    "            y_sub = y[sample_indexes]\n",
    "\n",
    "            decision_tree = DecisionTree(self.min_samples_split, 1, self.features, self.MOI)\n",
    "            decision_tree.fit(X_sub, y_sub)\n",
    "            predictions = decision_tree.predict(X)\n",
    "            predictions = np.where(predictions == 0, -1, 1)\n",
    "\n",
    "            y_bin = np.where(y == 0, -1, 1)\n",
    "            error = self.weighted_error(y_bin, predictions, sample_weights)\n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "            sample_weights *= np.exp(- alpha * y_bin * predictions)\n",
    "            sample_weights /= np.sum(sample_weights)\n",
    "\n",
    "            self.models.append(decision_tree)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        weighted_sum = np.zeros(len(X))\n",
    "        for model, alpha in zip(self.models, self.alphas):\n",
    "            pred = np.where(model.predict(X) == 0, -1, 1)\n",
    "            weighted_sum += alpha * pred\n",
    "        return np.where(weighted_sum >= 0, 1, 0)\n",
    "\n",
    "    def weighted_error(self, y_true, y_pred, sample_weights):\n",
    "        return np.sum(sample_weights[y_true != y_pred])"
   ],
   "id": "68423118989de594",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## load the database and separate it to training and testing",
   "id": "db2fb04a200354cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T18:23:51.858997Z",
     "start_time": "2025-06-25T18:23:51.072506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_fake_train_path = \"x_fake_train.npy\"\n",
    "X_real_train_path = \"x_real_train.npy\"\n",
    "X_fake_test_path = \"x_fake_test.npy\"\n",
    "X_real_test_path = \"x_real_test.npy\"\n",
    "X_train = np.concatenate((np.load(X_fake_train_path), np.load(X_real_train_path)), axis=0)\n",
    "X_test = np.concatenate((np.load(X_fake_test_path), np.load(X_real_test_path)), axis=0)\n",
    "y_train = np.array([0] * 50000 + [1] * 50000)\n",
    "y_test = np.array([0] * 10000 + [1] * 10000)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "X_test, y_test = shuffle(X_test, y_test, random_state=98)"
   ],
   "id": "3d6c07b5e907abc3",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# for a later use here is a accuracy calculation\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)"
   ],
   "id": "c89ec3e9c98a04c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
