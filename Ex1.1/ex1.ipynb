{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "◊û◊í◊ô◊©◊ô◊ù:\n",
    "◊ô◊®◊ì◊ü ◊ë◊ü ◊ò◊ú, ◊™.◊ñ: 308057785,\n",
    "◊ì◊†◊ô◊ê◊ú ◊õ◊î◊ü, ◊™.◊ñ: 211377932,\n",
    "◊ô◊ï◊°◊£ ◊õ◊î◊ü, ◊™.◊ñ: 208259002"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T18:23:49.448894Z",
     "start_time": "2025-05-07T18:23:49.444499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, log_loss,accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "e0db9a12e847f360",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T18:23:49.460914Z",
     "start_time": "2025-05-07T18:23:49.456901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def testmymodel(model, X_data, Y_data):\n",
    "    predictions = predict_with_model(model, X_data)\n",
    "\n",
    "    return accuracy_score(Y_data, predictions)\n"
   ],
   "id": "9e4a36a1e5f20680",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T18:23:49.483388Z",
     "start_time": "2025-05-07T18:23:49.479412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_with_model(model, X_data):\n",
    "    if isinstance(model, list):\n",
    "        probs = np.array([clf.predict_proba(X_data)[:, 1] for clf in model])\n",
    "        return np.argmax(probs, axis=0)\n",
    "    return model.predict(X_data)"
   ],
   "id": "5535e066450b8351",
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T18:23:49.511262Z",
     "start_time": "2025-05-07T18:23:49.500659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = np.load(\"cifar10_features.npy\")\n",
    "Y = np.load(\"cifar10_labels.npy\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ],
   "id": "8184e55a70276a13",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T18:23:51.339463Z",
     "start_time": "2025-05-07T18:23:49.528650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# OvR Logistic Regression\n",
    "start_time_ovr = time.time()\n",
    "model_ovr = []\n",
    "classes = list(set(y_train))\n",
    "\n",
    "for target_class in classes:\n",
    "    # Binary labels: 1 for current class, 0 for others\n",
    "    binary_labels = (y_train == target_class).astype(int)\n",
    "\n",
    "    clf = LogisticRegression(solver='lbfgs', max_iter=50000)\n",
    "    clf.fit(X_train, binary_labels)\n",
    "    model_ovr.append(clf)\n",
    "\n",
    "end_time_ovr = time.time()\n",
    "\n",
    "# Softmax Logistic Regression\n",
    "start_time_softmax = time.time()\n",
    "model_softmax = LogisticRegression(solver='newton-cg', max_iter=50000)\n",
    "model_softmax.fit(X_train, y_train)\n",
    "end_time_softmax = time.time()\n",
    "\n",
    "\n",
    "ovr_success = testmymodel(model_ovr, X_test, y_test)\n",
    "softmax_success = testmymodel(model_softmax, X_test, y_test)\n",
    "print(f\"OvR success rate: {ovr_success*100:.2f}%, OvR time: {end_time_ovr - start_time_ovr}\")\n",
    "print(f\"softmax success rate: {softmax_success*100:.2f}%, softmax time: {end_time_softmax - start_time_softmax}\")"
   ],
   "id": "aa8b0e82f8ad12a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OvR success rate: 96.10%, OvR time: 0.6329994201660156\n",
      "softmax success rate: 96.29%, softmax time: 1.158998966217041\n"
     ]
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Results ‚Äì Accuracy & Training Time\n",
    "| Model | Accuracy   | Training Time(s) |\n",
    "|-------|------------|------------------|\n",
    "| OvR   | **96.10%** | **0.6329**       |\n",
    "| Softmax | **96.28%** | **1.1589**       |\n",
    "\n",
    "#### Softmax attains a slightly higher accuracy (~0.0018 percentage‚Äëpoints) but requires ~75% more training time.\n"
   ],
   "id": "975ddc70e3488cd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T18:23:51.398623Z",
     "start_time": "2025-05-07T18:23:51.357650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_ovr_predict = predict_with_model(model_ovr, X_test)\n",
    "y_softmax_predict = predict_with_model(model_softmax, X_test)\n",
    "probs = np.array([model.predict_proba(X_test)[:, 1] for model in model_ovr])\n",
    "ovr_predict_proba = probs.T\n",
    "ovr_predict_proba /= ovr_predict_proba.sum(axis=1, keepdims=True)\n",
    "ovr_loss = log_loss(y_test, ovr_predict_proba)\n",
    "\n",
    "softmax_predict_proba = model_softmax.predict_proba(X_test)\n",
    "softmax_loss = log_loss(y_test, softmax_predict_proba)\n",
    "print(f\"OvR_loss: {ovr_loss:.4f}, softmax_loss: {softmax_loss:.4f}\")"
   ],
   "id": "65c9652aa0e0233b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OvR_loss: 0.1346, softmax_loss: 0.1077\n"
     ]
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Cross‚ÄëEntropy (Log‚ÄëLoss)\n",
    "| Model | Log‚ÄëLoss   |\n",
    "|-------|------------|\n",
    "| OvR   | **0.1346** |\n",
    "| Softmax | **0.1077** |\n",
    "\n",
    "#### A lower log‚Äëloss indicates better calibrated probability estimates hence Softmax provides more reliable confidence scores.\n"
   ],
   "id": "dd75b261d46c5818"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T18:23:51.432511Z",
     "start_time": "2025-05-07T18:23:51.416910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "f1_mean_ovr = f1_score(y_test, y_ovr_predict, average='macro')\n",
    "f1_mean_softmax = f1_score(y_test, y_softmax_predict, average='macro')\n",
    "print(f\"f1_mean_ovr: {f1_mean_ovr:.4f}\")\n",
    "print(f\"f1_mean_softmax: {f1_mean_softmax:.4f}\")"
   ],
   "id": "651b979dcef81761",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_mean_ovr: 0.9612\n",
      "f1_mean_softmax: 0.9631\n"
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### F1‚ÄëMacro\n",
    "| Model | F1‚ÄëMacro   |\n",
    "|-------|------------|\n",
    "| OvR   | **0.9612** |\n",
    "| Softmax | **0.9631** |\n",
    "\n",
    "#### The difference is small, but statistically relevant in a balanced multi-class task"
   ],
   "id": "dff8340ba9f90589"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T18:23:51.456921Z",
     "start_time": "2025-05-07T18:23:51.449745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conf_mat = confusion_matrix(y_test,y_ovr_predict)\n",
    "print(conf_mat)"
   ],
   "id": "438f7134e3bb0683",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1402    2   13    5    5    3    2   10   17    5]\n",
      " [   2 1464    4    1    0    1    0    1    5    7]\n",
      " [  11    1 1370   15   16   15    5    3    1    3]\n",
      " [  14    5   15 1450   12   51    5   10    3    4]\n",
      " [   6    0   13   15 1466    5    4    9    0    1]\n",
      " [   2    3   13   45   12 1437    8   11    2    1]\n",
      " [   5    5    9   17    4    6 1415    0    1    1]\n",
      " [   3    0    4   17    9    7    0 1456    0    1]\n",
      " [  13    2    1    5    1    0    2    0 1480    6]\n",
      " [  12    7    5    6    0    4    2    3    5 1475]]\n"
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Confusion Matrix (OvR)\n",
    "*Key misclassifications*\n",
    "- **Class 3 ‚Üí 5**: 45 samples\n",
    "- **Class 5 ‚Üí 3**: 51 samples\n",
    "\n",
    "#### These symmetric errors suggest classes 3 and 5 share similar feature representations.\n"
   ],
   "id": "8caf2334e24c4369"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T18:23:51.800792Z",
     "start_time": "2025-05-07T18:23:51.476920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mask = (y_train == 3) | (y_train == 5)\n",
    "X_pair = X_train[mask]\n",
    "Y_pair = y_train[mask]\n",
    "binary_model = LogisticRegression(solver='lbfgs', max_iter=50000)\n",
    "binary_model.fit(X_pair, Y_pair)\n",
    "\n",
    "refined_predictions = []\n",
    "\n",
    "for x_sample, y_pred in zip(X_test, y_ovr_predict):\n",
    "    if y_pred in [3, 5]:\n",
    "        # Use binary model only for confusing classes\n",
    "        refined_pred = binary_model.predict([x_sample])[0]\n",
    "        refined_predictions.append(refined_pred)\n",
    "    else:\n",
    "        # Keep original prediction\n",
    "        refined_predictions.append(y_pred)\n",
    "\n",
    "# Evaluate\n",
    "acc = accuracy_score(y_test, refined_predictions)\n",
    "f1 = f1_score(y_test, refined_predictions, average='macro')\n",
    "\n",
    "print(f\"Refined Accuracy: {acc*100:.2f}%\")\n",
    "print(f\"Refined F1-Mean: {f1:.4f}\")"
   ],
   "id": "a0e7716aab20240f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined Accuracy: 96.09%\n",
      "Refined F1-Mean: 0.9611\n"
     ]
    }
   ],
   "execution_count": 140
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Binary Refinement (Classes 3 vs 5)\n",
    "| Metric | Before Refinement | After Refinement |\n",
    "|--------|-------------------|------------------|\n",
    "| Accuracy | 96.10%            | 96.09%           |\n",
    "| F1‚ÄëMacro | 0.9612            | 0.9611           |\n",
    "\n",
    "#### The specialised binary classifier reduced the 3 ‚Üî 5 confusion marginally, but that gain was offset by new errors it introduced, leaving overall accuracy and F1‚Äëmacro **slightly lower**. This suggests:\n",
    "\n",
    "#### 1. **Sample‚Äëvolume effect** ‚Äì only ~0.6% of the test set involves classes 3 ‚Üî 5 misclassification correcting them cannot materially shift the macro metrics.\n",
    "#### 2. **Model overlap** ‚Äì the binary classifier learns essentially the same decision boundary already captured (imperfectly) by the original OvR models.\n",
    "#### 3. **Over‚Äëfitting risk** ‚Äì a classifier trained on the restricted subset may lack generalisation and occasionally override correct OvR decisions.\n",
    "\n",
    "#### Given the negligible improvement and added complexity, the original OvR ensemble remains the preferable solution for this task.\n",
    "\n"
   ],
   "id": "fd6344d806235a4a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üìä Final Performance Comparison\n",
    "\n",
    "| **Metric**             | **OvR**     | **Softmax** | **Difference** |\n",
    "|------------------------|-------------|-------------|----------------|\n",
    "| **Accuracy**           | 0.9610      | 0.9628      | 0.0018         |\n",
    "| **F1 Score (Macro)**   | 0.9612      | 0.9630      | 0.0018         |\n",
    "| **Log-Loss**           | 0.1345      | 0.1076      | 0.0269         |\n",
    "| **Training Time (sec)**| 0.6329      | 1.1589      | 0.5260         |\n",
    "\n",
    "---\n",
    "\n",
    "### üßæ Final Conclusion\n",
    "\n",
    "#### - Both models achieve **high and nearly identical performance** across all metrics (Accuracy and F1 Score differ by less than 0.2%).\n",
    "#### - **Softmax** consistently provides:\n",
    "   - #### **Slightly better classification quality** (higher F1, lower log-loss),\n",
    "   - #### But at the cost of **~2√ó slower training time**.\n",
    "#### - **OvR (One-vs-Rest)** is a strong, faster alternative with comparable results, making it preferable in runtime-sensitive scenarios.\n",
    "#### - The refinement experiment (targeting confusion between classes 3 & 5) yielded **no measurable improvement**, reinforcing that the original OvR model is already well-optimized.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Recommendation\n",
    "\n",
    "#### - Use **Softmax** if:\n",
    "   - #### You prioritize slightly higher prediction confidence and probability calibration.\n",
    "   - #### Training time is not a constraint.\n",
    "\n",
    "#### - Use **OvR** if:\n",
    "   - #### You prefer faster training and interpretable binary classifiers.\n",
    "   - #### The small drop in performance is acceptable for your application."
   ],
   "id": "ec6ab66f860ffea4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
